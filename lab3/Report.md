# æœºå™¨å­¦ä¹ æ¦‚è®ºå®éªŒæŠ¥å‘Š

**PB20111699 å´éªä¸œ**

**2022.11.20**

â€‹		åœ¨æœ¬æ¬¡çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬éœ€è¦è‡ªä¸»å®ç°åŸºäºå›å½’æ ‘çš„ XGBoost æ¨¡å‹ã€‚æœ¬æ–‡å°†ä»ç®—æ³•åŸç†ã€ä»£ç å®ç°ã€ç»“æœåˆ†æç­‰æ–¹é¢å±•å¼€ã€‚å¦‚æœå¯¹äºå…¶ä¸­éƒ¨åˆ†å†…å®¹æœ‰ç–‘é—®æˆ–è€…æœ‰å…¶ä»–æ”¹è¿›æ„è§ï¼Œæ¬¢è¿æå‡º issue æˆ–ç›´æ¥è”ç³»ä½œè€…ã€‚



## åŸç†åˆ†æ

### XGBoost

XGBoost æ˜¯ç”±å¤šä¸ªåŸºæ¨¡å‹ç»„æˆçš„ä¸€ä¸ªåŠ æ³•æ¨¡å‹ï¼Œå‡è®¾ç¬¬ $k$ ä¸ªåŸºæœ¬æ¨¡å‹æ˜¯ $f_k (x)$, é‚£ä¹ˆå‰ $t$ ä¸ªæ¨¡å‹ç»„æˆçš„æ¨¡å‹çš„è¾“å‡ºä¸º
$$
y Ì‚_i^{(t)}=âˆ‘^t_{k=1}f_k (x_i )=y Ì‚_i^{(t-1)}+f_t (x_i )
$$
å…¶ä¸­ $x_i$ ä¸ºç¬¬è¡¨ç¤ºç¬¬ $i$ ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ$y_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾;  $y Ì‚_i^{(t)}$ è¡¨ç¤ºå‰ $t$ ä¸ªæ¨¡å‹å¯¹ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æ ‡ç­¾æœ€ç»ˆé¢„æµ‹å€¼ã€‚

åœ¨å­¦ä¹ ç¬¬ $t$ ä¸ªåŸºæ¨¡å‹æ—¶ï¼ŒXGBoost è¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ä¸º:
$$
\begin{split}
Obj^{(t)} &= \sum_{i=1}^n loss(y_i,\hat y_i^{(t)})+\sum_{k=1}^t penalty(f_k)\\
&=\sum_{i=1}^n loss(y_i,\hat y_i^{(t-1)}+f_t(x_i))+\sum_{k=1}^t penalty(f_k)\\
&=\sum_{i=1}^n loss(y_i,\hat y_i^{(t-1)}+f_t(x_i))+ penalty(f_t)+constant\\
\end{split}
$$
å…¶ä¸­ $n$ è¡¨ç¤ºè®­ç»ƒæ ·æœ¬çš„æ•°é‡, $penalty(f_k)$ è¡¨ç¤ºå¯¹ç¬¬ $k$ ä¸ªæ¨¡å‹çš„å¤æ‚åº¦çš„æƒ©ç½šé¡¹,  $loss(y_i,\hat y_i^{(t)})$ è¡¨ç¤ºæŸå¤±å‡½æ•°,

ä¾‹å¦‚äºŒåˆ†ç±»é—®é¢˜çš„ 
$$
ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦_ğ‘–,ğ‘¦ Ì‚_ğ‘–^{(ğ‘¡)} )=âˆ’ğ‘¦_ğ‘–\cdot \logâ¡ p(ğ‘¦ Ì‚_ğ‘–^{(t)}=1|ğ‘¥_ğ‘–)âˆ’(1âˆ’ğ‘¦_ğ‘–)\logâ¡ (1-p(y Ì‚_ğ‘–^{(t)}=1|ğ‘¥_ğ‘–))
$$
å›å½’é—®é¢˜
$$
ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦_ğ‘–,\hat{y}^{(t)}_i )=(ğ‘¦_ğ‘–âˆ’\hat{y}^{(t)}_i )^2
$$
å°† $loss(y_i,\hat{y}^{(t-1)}_i+f_t (x_i))$ åœ¨ $y Ì‚_i^{(t-1)}$ å¤„æ³°å‹’å±•å¼€å¯å¾—
$$
loss(y_i,y Ì‚_i^{(t-1) }+f_t (x_i))â‰ˆloss(y_i,y Ì‚_i^{(t-1)} )+g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)
$$
å…¶ä¸­ $g_i=\frac{\partial\ loss(y_i,y Ì‚_i^{(t-1)})}{\partial\  y Ì‚_i^{(t-1) } }$, $h_i=\frac{\partial^2 loss(y_i,y Ì‚_i^{(t-1)} )}{\partial \ (y Ì‚_i^{(t-1)} )^2 }\\ $ï¼Œå³ $g_i$ ä¸ºä¸€é˜¶å¯¼æ•°ï¼Œ$h_i$ ä¸ºäºŒé˜¶å¯¼æ•°ã€‚

æ­¤æ—¶çš„ä¼˜åŒ–ç›®æ ‡å˜ä¸º
$$
Obj^{(t)}=âˆ‘_{i=1}^n[loss(y_i,y Ì‚_i^{(t-1)} )+g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)]+penalty(f_t ) +constant
$$
å»æ‰å¸¸æ•°é¡¹ $loss(y_i,y Ì‚_i^{(t-1) })$ (å­¦ä¹ ç¬¬ $t$ ä¸ªæ¨¡å‹æ—¶å€™ï¼Œ $loss(y_i,y Ì‚_i^{(t-1) })$ ä¹Ÿæ˜¯ä¸€ä¸ªå›ºå®šå€¼) å’Œ constantï¼Œå¯å¾—ç›®æ ‡å‡½æ•°ä¸º
$$
Obj^{(t)}=âˆ‘_{i=1}^n[g_i f_t (x_i )+\frac12 h_i f_t^2 (x_i)]+penalty(f_t )
$$



### å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰

â€‹		æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»¥å†³ç­–æ ‘ï¼ˆå›å½’æ ‘ï¼‰ä¸ºåŸºã€‚å‡è®¾å†³ç­–æ ‘æœ‰ $T$ ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹å¯¹åº”æœ‰ä¸€ä¸ªæƒé‡ã€‚å†³ç­–æ ‘æ¨¡å‹å°±æ˜¯å°†è¾“å…¥ $x_i$ æ˜ å°„åˆ°æŸä¸ªå¶å­èŠ‚ç‚¹ï¼Œå†³ç­–æ ‘æ¨¡å‹çš„è¾“å‡ºå°±æ˜¯è¿™ä¸ªå¶å­èŠ‚ç‚¹çš„æƒé‡ï¼Œå³ $f(x_i )=w_q(x_i )$ ï¼Œ$w$ æ˜¯ä¸€ä¸ªè¦å­¦çš„ $T$ ç»´çš„å‘é‡ã€‚å…¶ä¸­ $q(x_i)$ è¡¨ç¤ºæŠŠè¾“å…¥ $x_i$ æ˜ å°„åˆ°çš„å¶å­èŠ‚ç‚¹çš„ç´¢å¼•ï¼Œä¾‹å¦‚ï¼š$q(x_i )=3$ï¼Œé‚£ä¹ˆæ¨¡å‹è¾“å‡ºç¬¬ä¸‰ä¸ªå¶å­èŠ‚ç‚¹çš„æƒé‡ï¼Œå³ $f(x_i )=w_3$ã€‚

â€‹		å¯¹äºæŸä¸€æ£µå†³ç­–æ ‘ï¼Œæˆ‘ä»¬å®šä¹‰ä»–çš„æƒ©ç½šä¸º
$$
penalty(f)=\gamma\cdot T+\frac12\lambda\cdot\|w\|^2
$$
å…¶ä¸­ $\gamma,\lambda$ ä¸ºå¯è°ƒæ•´çš„è¶…å‚æ•°ï¼Œ$T$ ä¸ºå¶å­ç»“ç‚¹æ•°ï¼Œ$w$ ä¸ºæƒé‡å‘é‡. ç”±äºæ˜¾ç¤ºé—®é¢˜ï¼Œ$\|w\|$ å®é™…ä¸Šä¸º $w$ çš„èŒƒæ•°ï¼Œä¸” $\|w\|^2=\sum_{i=1}^{dim}w_i^2$ã€‚

â€‹		æˆ‘ä»¬å°†åˆ†é…åˆ°ç¬¬ $j$ ä¸ªå¶å­èŠ‚ç‚¹çš„æ ·æœ¬ç”¨ $I_j$ è¡¨ç¤ºï¼Œå³ $I_j=\{i|q(x_i )=j\} (1â‰¤jâ‰¤T)$ã€‚åˆ™åœ¨æ ‘ç»“æ„ç¡®å®šæ—¶ï¼Œå¯ä»¥è¿›è¡Œå¦‚ä¸‹ä¼˜åŒ–ï¼š
$$
\begin{split}
ğ‘‚ğ‘ğ‘—^{(ğ‘¡)}&=âˆ‘_{ğ‘–=1}^ğ‘›[ğ‘”_ğ‘– ğ‘“_ğ‘¡ (ğ‘¥_ğ‘– )+\frac12 â„_ğ‘– ğ‘“_ğ‘¡^2 (ğ‘¥_ğ‘–)]+ğ‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘¡ğ‘¦(ğ‘“_ğ‘¡ )\\
&= âˆ‘_{i=1}^n[g_iw_{q(ğ‘¥_ğ‘– )} +\frac12  â„_ğ‘– ğ‘¤_{ğ‘(ğ‘¥_ğ‘– )}^2]+ğ›¾â‹…ğ‘‡+\frac12 ğœ†â‹…\|ğ‘¤\|^2\\
&=âˆ‘_{ğ‘—=1}^ğ‘‡[(âˆ‘_{iâˆˆğ¼_ğ‘—}ğ‘”_ğ‘– )â‹…ğ‘¤_ğ‘—+\frac12â‹…(âˆ‘_{ğ‘–âˆˆğ¼_ğ‘—}â„_ğ‘–+ğœ†)â‹…ğ‘¤_ğ‘—^2 ]+ğ›¾â‹…ğ‘‡
\end{split}
$$
ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç®€è®° $G_j=âˆ‘_{iâˆˆğ¼_ğ‘—}ğ‘”_ğ‘– , H_j=âˆ‘_{iâˆˆğ¼_ğ‘—}h_ğ‘– $
$$
Obj^{(t)}=\sum_{j=1}^T[G_jw_j+\frac12(H_j+\lambda)w_j^2]+\gamma T
$$

ä»¤ä¸Šå¼å…³äº $w_j$ çš„æ¢¯åº¦ä¸º 0ï¼Œæˆ‘ä»¬å¯å¾—
$$
\frac{\partial Obj^{(t)}}{\partial w_j}=G_j+(H_j+\lambda)w_j=0\\
\Rightarrow w_j=-\frac{G_j}{H_j+\lambda}
$$
è¿™å°±æ˜¯æœ€ä¼˜æƒ…å†µä¸‹çš„ $w_j$ã€‚å¯¹åº”æœ€ä¼˜ $Obj^{(t)}$ ï¼ˆå½“å‰æ¨¡å‹çš„å¾—åˆ†ï¼‰ä¸º
$$
Obj^{(t)}=\sum_{j=1}^T-\frac{G_j^2}{2(H_j+\lambda)}+\gamma T
$$


â€‹		å¯¹äºå›å½’æ ‘æ¨¡å‹ï¼Œæˆ‘ä»¬æœ‰
$$
ğ‘™ğ‘œğ‘ ğ‘ (ğ‘¦_ğ‘–,\hat{y}^{(t)} )=(\hat{y}^{(t)})^2
$$
æ‰€ä»¥
$$
g_i = 2(y_i-\hat{y}^{(t)}_i)\\
h_i=2
$$
æ•…
$$
G_j=âˆ‘_{iâˆˆğ¼_ğ‘—}2(y_i-\hat{y}^{(t)}_i) \\
H_j=2|I_j|
$$








## ä»£ç è®¾è®¡ä¸åˆ†æ

### å›å½’æ ‘

#### åŸºæœ¬æ•°æ®ç»“æ„

â€‹		å¯¹äºä¸€æ£µå›å½’æ ‘ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨äºŒå‰æ ‘çš„åŸºæœ¬ç»“æ„ï¼Œå†åŠ ä¸Šä¸€äº›é¢å¤–çš„ä¿¡æ¯ä½œä¸ºæ•°æ®ç»“æ„ã€‚ä¾‹å¦‚ç»“ç‚¹ç¼–å·ã€ç»“ç‚¹çš„é¢„æµ‹å€¼ï¼ˆå¶å­ç»“ç‚¹ä¸“å±ï¼‰ã€ç»“ç‚¹å¯¹åº”çš„æ ·æœ¬é›†åˆã€ç»“ç‚¹çš„ç‰¹å¾åˆ†ç±»æ ‡å‡†ç­‰ã€‚

```python
class Node:
    def __init__(self, index:list, id = -1, feature = -1, f_val = -1.) -> None:
        self.l = -1
        self.r = -1
        self.id = id
        self.w = -1

        self.index = index
        self.feature = feature
        self.f_val = f_val
```

è€ƒè™‘åˆ° python çš„è¯­è¨€ç‰¹æ€§ï¼Œæˆ‘ä»¬æ— éœ€é‡‡ç”¨æŒ‡é’ˆæˆ–é¢å¤–çš„æ•°æ®ç»“æ„æ¥è¿›è¡Œç©ºç»“ç‚¹çš„åˆ¤æ–­ï¼Œç›´æ¥èµ‹å€¼ä¸º -1 å³å¯ã€‚



#### è¾“å‡ºé¢„æµ‹

â€‹		å›å½’æ ‘çš„é¢„æµ‹å¯ä»¥çœ‹ä½œæ˜¯å¯¹äºäºŒå‰æ ‘çš„æŸ¥æ‰¾æ“ä½œã€‚ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œæ ¹æ®æ¯ä¸ªç»“ç‚¹çš„åˆ†ç±»æ ‡å‡†é€‰æ‹©è¿›å…¥å·¦å³å­©å­ï¼Œç›´åˆ°æ¥åˆ°å¶å­ç»“ç‚¹ï¼Œå¹¶å°†å¶å­ç»“ç‚¹çš„ w å€¼ä½œä¸ºé¢„æµ‹å€¼è¾“å‡ºã€‚

```python
def _predict(self, x): 
    # Predict a sample
    node = self.root
    while node.l != -1 or node.r != -1:
        if x[node.feature] <= node.f_val:       
            node = node.l
        else:
            node = node.r
    return node.w
```



#### å‚æ•°ä¼ é€’

â€‹		è€ƒè™‘åˆ°å¦‚æœç›´æ¥ä¼ é€’åŸå§‹æ•°æ®é›†æˆ–å…¶å­é›†ï¼Œæˆ‘ä»¬ä¼šæ¶ˆè€—ååˆ†å·¨å¤§çš„ç©ºé—´èµ„æºã€‚ä½†å¯¹äºæ¯ä¸ªç»“ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»è¦è®°å½•å½“å‰èŠ‚ç‚¹æ‰€åˆ†é…åˆ°çš„æ ·æœ¬ç¼–å·ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‹æ ‡ä¼ é€’ç­–ç•¥ã€‚è€ƒè™‘å¦‚ä¸‹è¯­æ³•

```python
arr = np.array([5,4,3,2,1])
index = [3,1,0]
print(arr[index])
>>> [2, 4, 5]
```

â€‹		è¿™æ ·ï¼Œæˆ‘ä»¬å°±åªéœ€è¦åœ¨æ¯ä¸ªç»“ç‚¹ç»´æŠ¤å½“å‰çš„ index åˆ—è¡¨å³å¯ï¼ŒèŠ‚çœäº†ç©ºé—´ï¼ŒåŒæ—¶åˆå®ç°äº†æ—¢å®šçš„ç›®æ ‡ã€‚



#### ç¡®å®šæœ€ä¼˜åˆ’åˆ†

â€‹		å¯¹äºæ¯ä¸€æ£µå†³ç­–æ ‘ï¼Œå³æ¯ä¸€ä¸ªåŸºçš„è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ’åˆ†ç»“ç‚¹

1. ä»æ ¹èŠ‚ç‚¹å¼€å§‹é€’å½’åˆ’åˆ†ï¼Œåˆå§‹æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ $x_i$ éƒ½åˆ†é…ç»™æ ¹èŠ‚ç‚¹ã€‚

2. æ ¹æ®åˆ’åˆ†å‰åçš„æ”¶ç›Šåˆ’åˆ†ç»“ç‚¹ï¼Œæ”¶ç›Šä¸º
   $$
   Gain = Obj_P-Obj_L-Obj_R
   $$
   å…¶ä¸­ $Obj_P$ ä¸ºçˆ¶ç»“ç‚¹çš„å¾—åˆ†ï¼Œ$Obj_L,Obj_R$ ä¸ºå·¦å³å­©å­çš„å¾—åˆ†.

3. é€‰æ‹©æœ€å¤§å¢ç›Šè¿›è¡Œåˆ’åˆ†

é€‰æ‹©æœ€å¤§å¢ç›Šçš„è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. é€‰å‡ºæ‰€æœ‰å¯ä»¥ç”¨æ¥åˆ’åˆ†çš„ç‰¹å¾é›†åˆ $\mathcal F$ï¼›
2. For feature in $\mathcal F$:
3. å°†èŠ‚ç‚¹åˆ†é…åˆ°çš„æ ·æœ¬çš„ç‰¹å¾ feature æå–å‡ºæ¥å¹¶å‡åºæ’åˆ—ï¼Œè®°ä½œ sorted_f_value_listï¼›
4. For f_value in sorted_f_value_list ï¼š
5. åœ¨ç‰¹å¾ feature ä¸ŠæŒ‰ç…§ f_value ä¸ºä¸´ç•Œç‚¹å°†æ ·æœ¬åˆ’åˆ†ä¸ºå·¦å³ä¸¤ä¸ªé›†åˆï¼›
6. è®¡ç®—åˆ’åˆ†åçš„å¢ç›Šï¼›
7. è¿”å›æœ€å¤§çš„å¢ç›Šæ‰€å¯¹åº”çš„ feature å’Œ f_valueã€‚ 

ä¸Šé¢æ˜¯ç®—æ³•çš„æµç¨‹å›¾ã€‚æœ¬å®éªŒçš„ä»£ç å®ç°å¦‚ä¸‹

```python
for col in range(self.n):
    _feature = self.X[node.index, col:col+1].copy().reshape(1, -1)[0]

    feature = list(set(_feature))	# æ¶ˆé™¤ç›¸åŒå€¼
    feature.sort()

    for f_value in feature:
        _index1 = np.where((self.X[node.index, col:col+1] <= f_value).all(axis=1))[0]
        index1 = np.array(node.index)[_index1]
        _index2 = np.where((self.X[node.index, col:col+1] > f_value).all(axis=1))[0]
        index2 = np.array(node.index)[_index2]
        
        gain = self._get_score(node.index, self.leaf_num) \
             - self._get_score(index1, self.leaf_num + 1) \
             - self._get_score(index2, self.leaf_num + 1)

        if gain > max_gain:
            # æ›´æ–°å½“å‰è®°å½•çš„æœ€ä¼˜åˆ’åˆ†ä¿¡æ¯
            max_gain = gain
            max_feature = col
            max_f_value = f_value
            max_index1 = index1
            max_index2 = index2

      
return max_feature, max_f_value, max_gain, max_index1, max_index2
```

â€‹		æ³¨æ„åˆ°æˆ‘ä»¬é‡‡ç”¨äº†ä¸‹æ ‡ä½œä¸ºæ ·æœ¬é›†åˆçš„ä¼ é€’å‚æ•°ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±éœ€è¦å¾—åˆ°åˆ’åˆ†ä¹‹åçš„ä¸‹æ ‡é›†åˆï¼Œä¹Ÿå°±æ˜¯åŸå§‹æ•°æ®é›†ä¸­**æŸä¸€åˆ—å°äºç‰¹å®šå€¼çš„è¡Œçš„ä¸‹æ ‡**é›†åˆã€‚æ€è·¯ä¹Ÿå¾ˆç›´æ¥ï¼šå¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬å•ç‹¬è¿›è¡Œæ£€æµ‹å³å¯ã€‚ä½† numpy æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„å‡½æ•°å®ç° `np.where()` [å‚è€ƒï¼šnp.where() çš„ä½¿ç”¨æ–¹æ³•](https://blog.csdn.net/island1995/article/details/90200151)ã€‚

```python
_index = np.where((self.X[node.index, col:col+1] <= f_value).all(axis=1))[0]
index = np.array(node.index)[_index]
```

â€‹		ç¬¬ä¸€è¡Œç¡®å®šäº†ä¸‹æ ‡åœ¨ node.index ä¸­çš„è¡Œï¼ˆ**å½“å‰ç»“ç‚¹æ‰€åŒ…å«çš„æ ·æœ¬**ï¼‰çš„ col åˆ—ä¸­æ‰€æœ‰ä¸‹æ ‡ï¼ˆ**å½“å‰ç»“ç‚¹åŒ…å«çš„æ ·æœ¬åœ¨å½“å‰ç‰¹å¾çš„å–å€¼**ï¼‰å°äºå½“å‰åˆ’åˆ†æ ‡å‡†çš„è¡Œçš„ä¸‹æ ‡ï¼ˆæ ·æœ¬ç¼–å·ï¼‰ã€‚ç”±äºè¿™é‡Œæ˜¯åœ¨å­é›†ä¸­è¿›è¡Œçš„è®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦å°†ä¸‹æ ‡å†æ˜ å°„å›åˆ°ç»“ç‚¹åŒ…å«æ ·æœ¬çš„ä¸‹æ ‡ä¸Šï¼Œè¿™ä¹Ÿå°±æ˜¯ç¬¬äºŒè¡Œçš„ä½œç”¨ã€‚



#### å›å½’æ ‘çš„æ„å»º

â€‹		æ„å»ºä¸€æ£µå›å½’æ ‘ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡ç”¨é€’å½’çš„ç­–ç•¥ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰ï¼Œå³ä¼˜å…ˆå¯¹å·¦å­æ ‘è¿›è¡Œåˆ’åˆ†ï¼Œç›´åˆ°å·¦å­æ ‘ç§°ä¸ºå¶å­ç»“ç‚¹ä¸ºæ­¢ã€‚

```python
def _fit(self, node:Node, depth = 1)->int:
      
    feature, f_value, gain, index1, index2 = self._get_best_split(node)
    # è·å¾—å½“å‰ç»“ç‚¹çš„æœ€ä¼˜åˆ’åˆ†ä¿¡æ¯
    _feature = self.X[node.index, feature:feature+1].copy().reshape(1, -1)[0]
    f = list(set(_feature))

    if < ç‰¹å®šçš„åˆ’åˆ†åœæ­¢æ¡ä»¶ >:
        # This is a leaf
        # Do something else
        pass

    
    node.feature = feature
    node.f_val = f_value
    # è®¾å®šå½“å‰ç»“ç‚¹çš„åˆ’åˆ†ä¿¡æ¯
    node.l = Node(index1, id=self.node_num)
    node.r = Node(index2, id=self.node_num + 1)
    self.node_num += 2

    self._fit(node.l, depth+1)
    self._fit(node.r, depth+1)
```

â€‹		è‡ªç„¶ï¼Œä½¿ç”¨å¹¿åº¦ä¼˜å…ˆä¹Ÿæ˜¯å¯è¡Œçš„ï¼Œåªéœ€è¦ä½¿ç”¨ä¸€ä¸ªé˜Ÿåˆ—ç»´æŠ¤å¾…å¤„ç†çš„ç»“ç‚¹å³å¯ã€‚



#### â˜…åˆ’åˆ†åœæ­¢çš„ç­–ç•¥

â€‹		ç°åœ¨ï¼Œæˆ‘ä»¬å°†è®¨è®ºå›å½’æ ‘æ¨¡å‹ä¸­æœ€é‡è¦çš„é—®é¢˜ï¼šå¦‚ä½•å†³å®šåœæ­¢åˆ’åˆ†ï¼Œå°†å½“å‰ç»“ç‚¹æ ‡è®°ä¸ºå¶å­èŠ‚ç‚¹ï¼Œå› ä¸ºæ— ä¼‘æ­¢çš„åˆ’åˆ†å°†ä¼šå¯¼è‡´ç¾éš¾æ€§çš„è¿‡æ‹Ÿåˆã€‚ä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬é™åˆ¶å›å½’æ ‘çš„æ·±åº¦å³å¯ã€‚æœ¬å®éªŒå¼•å…¥äº†å¦‚ä¸‹çš„ç»ˆæ­¢ç­–ç•¥

- æ ‘çš„æœ€å¤§æ·±åº¦ï¼šä¸€æ£µæ ‘è‡ªç„¶ä¸èƒ½å¤ªæ·±ï¼Œä¸ºæ­¤æˆ‘ä»¬é™åˆ¶å›å½’æ ‘çš„æ·±åº¦ï¼Œå½“å½“å‰ç»“ç‚¹æ·±åº¦è¾¾åˆ°è®¾å®šé˜ˆå€¼æ—¶ï¼Œå°±å°†å…¶æ ‡è®°ä¸ºå¶å­ç»“ç‚¹ï¼›
- ç»“ç‚¹çš„æœ€å°‘æ ·æœ¬æ•°ï¼šä»…æœ‰ä¸€ä¸ªæ ·æœ¬çš„ç»“ç‚¹è‡ªç„¶å¯ä»¥ç§°ä¸ºå¶å­ç»“ç‚¹ã€‚ä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥äººä¸ºè®¾å®šè¿™ä¸€é˜ˆå€¼ï¼Œä½¿å¾—å°‘äºç‰¹å®šæ•°é‡çš„æ ·æœ¬å³ä¼šè¢«è§†ä½œä¸€ä¸ªå•ç‹¬çš„å¶å­ç»“ç‚¹è€Œä¸è¿›è¡Œåˆ’åˆ†ï¼›
- ç»“ç‚¹çš„æœ€å°‘ä¸åŒç‰¹å¾æ•°ï¼šæ ·æœ¬çš„ç‰¹å¾å‡ä¸ºè¿ç»­å€¼ï¼Œè¿™æ ·åˆ’åˆ†åˆ°æœ€åæ€»æ˜¯å¾—åˆ°å•ä¸€ç‰¹å¾å–å€¼çš„å¶å­ç»“ç‚¹ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥äººä¸ºå°†å…¶ç¦»æ•£åŒ–ï¼Œå½“è¯¥åˆ’åˆ†ç‰¹å¾çš„å¯èƒ½å–å€¼æ•°ç›®å°äºä¸€å®šé˜ˆå€¼æ—¶ï¼Œå°±ä¸å†è¿›è¡Œåˆ’åˆ†ï¼›
- ç»“ç‚¹çš„æœ€å°å¢ç›Šï¼šå½“åˆ’åˆ†å‰åèŠ‚ç‚¹å¾—åˆ†å¢ç›Šä¸è¶³æ—¶ï¼Œæˆ‘ä»¬å°±ä¸å†è¿›è¡Œåˆ’åˆ†ã€‚è‡ªç„¶ï¼Œç»“ç‚¹çš„æœ€å°å¢ç›Šåº”å½“è‡³å°‘ä¸º 0ï¼Œå¦åˆ™å›å½’æ ‘å°†è¢«è´Ÿä¼˜åŒ–ã€‚





### XGBoost

â€‹		ç›¸æ¯”è¾ƒå›å½’æ ‘ï¼ŒXGBoost ä»…ä»…æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ‰€ä»¥å®ç°èµ·æ¥å°±ç®€å•äº†å¾ˆå¤šã€‚

```python
def fit(self, T, min_train_err,tree_parameters):
    m, n = self.y.shape
    self.T = T
    y_t = np.zeros((m, 1))
	# åˆå§‹æ¨¡å‹è¾“å‡ºè®¾ä¸º 0
    for _ in range(0, self.T):
        tree = RegTree(self.X, self.y, y_t)
        tree.fit(tree_parameters)

        y_t = y_t + tree.predict(self.X)
        self.treeset.append(tree)
        
        err = tree._get_err(y_t)
        # è®¡ç®—æ˜¯å¦äº§ç”Ÿè¿‡æ‹Ÿåˆ
        if err < min_train_err:
            break
```

å¯¹äºæ¯ä¸€è½®çš„è¿­ä»£ï¼Œæˆ‘ä»¬åªéœ€è¦ç”¨ä¸Šä¸€è½®çš„ç»“æœé‡æ–°è®­ç»ƒåŸºå­¦ä¹ å™¨ï¼Œå¹¶å°†ç»“æœè¿›è¡Œå åŠ å³å¯ã€‚

#### â˜…è¿­ä»£åœæ­¢çš„ç­–ç•¥

â€‹		ä¸Šå±‚æ¨¡å‹æä¾›çš„è¿­ä»£åœæ­¢ç­–ç•¥åŒ…æ‹¬ï¼š

- åŸºå­¦ä¹ å™¨çš„æ•°ç›®ï¼šåœ¨å­¦ä¹  T ä¸ªåŸºå­¦ä¹ å™¨çš„è¾“å‡ºåå°±åœæ­¢ã€‚è¿™æ˜¯ä¸ºäº†é˜²æ­¢æ¬ æ‹Ÿåˆï¼›
- éªŒè¯é›†å‡æ–¹è¯¯å·®ï¼šåœ¨éªŒè¯é›†ä¸Šçš„å‡æ–¹è¯¯å·®å°äºæŸä¸ªé˜ˆå€¼æ—¶åœä¸‹æ¥ã€‚è¿™æ˜¯ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆã€‚





## æ¨¡å‹è®­ç»ƒä¸æ¯”è¾ƒ

### è¯„ä»·æŒ‡æ ‡

â€‹		æœ¬å®éªŒä¸­é‡‡ç”¨çš„æŒ‡æ ‡åŒ…æ‹¬ï¼š

- $RMSE=\sqrt{\frac1m\sum_{i=1}^m(y_{test}^{(i)}-\hat y_{test}^{(i)})^2}\\ $ï¼Œè¶Šå°è¶Šå¥½ï¼Œ

- $R^2=1-\frac{ \sum_{i=1}^m(y_{test}^{(i)}-\hat y_{test}^{(i)})^2}{\sum_{i=1}^m(\bar y_{test}-\hat y_{test}^{(i)})^2}=1-\frac{MSE(\hat y_{test},y_{test})}{Var(y_{test})}\\ $ï¼Œè¶Šå¤§è¶Šå¥½


```python
def RMSE(self, pre:np.ndarray, val:np.ndarray):
    y = val - pre
    m, _ = y.shape
    return np.sqrt((y.T @ y) / m)[0][0]

def R2(self, pre:np.ndarray, val:np.ndarray):
    var = np.var(val)
    return(1 - (self.RMSE(pre, val) ** 2)/ var)
```



### å•ä¸€å›å½’æ ‘

é»˜è®¤å‚æ•°é›†åˆä¸º

```python
parameters = {
    "lamda": 1,            
    "gamma": 0,            
    "gain_delta": 0,       
    "max_depth": 10,
    "max_leaves": 100,
    "max_nodes": 1000,
    "min_samples": 3,      
    "min_feature_dif": 3, 
    "if_silent": 1,
}
```

#### è¶…å‚æ•° $\gamma$

<img src="./pics/single_gamma.png" alt="image-20221119215206181" style="zoom: 80%;" />

#### è¶…å‚æ•° $\lambda$

<img src="./pics/single_lambda.png" alt="image-20221119220023553" style="zoom:80%;" />

#### æ ‘çš„æœ€å¤§æ·±åº¦

<img src="./pics/single_tree_height.png" alt="image-20221119220023553" style="zoom:80%;" />

â€‹		å½“æ·±åº¦è¶…è¿‡ 12 æ—¶æ›²çº¿ä¸å†å˜åŒ–ï¼Œè¡¨æ˜å·²ç»å‡ºç°äº†è¿‡æ‹Ÿåˆã€‚

#### å¶å­ç»“ç‚¹æ ·æœ¬æ•°

<img src="./pics/single_samples.png" alt="image-20221119220023553" style="zoom:80%;" />

#### æœ€ç»ˆæ¨¡å‹

â€‹		é€‰å–å‚æ•°ä¸º

```python
parameters = {
    "lamda": 0.1,            
    "gamma": 0,            
    "gain_delta": 0,       
    "max_depth": 9,
    "max_leaves": 100,
    "max_nodes": 1000,
    "min_samples": 5,      
    "min_feature_dif": 3, 
    "if_silent": 1,
}
```

å¾—åˆ°çš„å•å›å½’æ ‘æŒ‡æ ‡ä¸ºï¼šRMSE = 0.0002ï¼ŒR2 = 0.7558ã€‚

<img src="./pics/single_tree.png" alt="image-20221119222155567" style="zoom: 67%;" />

<img src="./pics/single_tree_RMSE.png" alt="image-20221119222155567" style="zoom: 80%;" />

<img src="./pics/single_tree_loss.png" alt="image-20221119222155567" style="zoom: 80%;" />

RMSE ä¸ å‡æ–¹è¯¯å·®çš„è®­ç»ƒæ›²çº¿åŸºæœ¬ä¸€è‡´ã€‚



### XGBoost æ¨¡å‹

é»˜è®¤å‚æ•°ï¼šT = 30ï¼Œmin_err = 1e-5ï¼Œ

```python
parameters = {
    "lamda": 0.1,            
    "gamma": 0,            
    "gain_delta": 0,       
    "max_depth": 3,
    "max_leaves": 100,
    "max_nodes": 1000,
    "min_samples": 3,      
    "min_feature_dif": 3, 
    "if_silent": 1,
}
```

#### å›å½’æ ‘æ•°ç›®

| åŸºå­¦ä¹ å™¨æ•°ç›® |          RMSE           |   R2    |
| :----------: | :---------------------: | :-----: |
|      5       | $2.24495 \times10^{-4}$ | 0.71653 |
|      15      | $2.03415 \times10^{-4}$ | 0.76727 |
|      30      | $1.96534 \times10^{-4}$ | 0.78275 |
|      50      | $1.80369 \times10^{-4}$ | 0.79752 |



#### æ ·æœ¬å‡æ–¹è¯¯å·®

| æœ€å°æ ·æœ¬å‡æ–¹è¯¯å·® |          RMSE           |   R2    |
| :--------------: | :---------------------: | :-----: |
|       1e-2       | $2.80434 \times10^{-4}$ | 0.52902 |
|       1e-4       | $1.90967 \times10^{-4}$ | 0.78159 |
|       1e-6       | $1.90967 \times10^{-4}$ | 0.78159 |
|       1e-8       | $1.90967 \times10^{-4}$ | 0.78159 |



#### æœ€ç»ˆæ¨¡å‹

T = 35ï¼Œ min_err = 1e-4ã€‚

<img src="./pics/XGBoost.png" alt="image-20221119224934749" style="zoom:67%;" />

<img src="./pics/XGBoost_err.png" alt="image-20221119224934749" style="zoom: 80%;" />

å…¶ä¸­ï¼Œçºµåæ ‡ä¸ºè®­ç»ƒé›†å‡æ–¹è¯¯å·®ã€‚R2 è¾¾åˆ°äº† 0.8+ï¼Œä¸ Sklearn æ‰€æä¾›çš„åº“æ•ˆæœç›¸å½“ã€‚

â€‹		

### æ€»ç»“

â€‹		å¯ä»¥çœ‹åˆ°ï¼Œé‡‡ç”¨ XGBoost å¯ä»¥åœ¨ä»…è®­ç»ƒ 3 å±‚çš„å›å½’æ ‘åŸºç¡€ä¸Šå¾—åˆ°å›å½’æ•ˆæœè¾ƒå¥½çš„é¢„æµ‹æ¨¡å‹ã€‚è¿™å°±æ˜¯é›†æˆå­¦ä¹ çš„åŠ›é‡ï¼