{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd \n",
    "import pickle as pk\n",
    "\n",
    "from Judge.Score import *   # 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.read_csv('./Datasets/train_feature.csv')\n",
    "df_lb = pd.read_csv('./Datasets/train_label.csv') \n",
    "\n",
    "# df_origin = pd.concat([df_ft_origin, df_lb_origin],axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "首先进行数据的预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(df_ft_origin.head())\n",
    "# print(df_ft_origin['feature_0'])\n",
    "# print(type(df_ft_origin['feature_0']))\n",
    "# print(df_ft_origin.mean())\n",
    "# print(df_ft_origin.median())\n",
    "# print(df_ft_origin.var())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5195, 121)\n",
      "(5195, 13)\n",
      "   feature_9  feature_35  feature_43  feature_48  feature_67  feature_92  \\\n",
      "0   0.040544    0.003153    0.027659    0.058805    0.014114    0.838889   \n",
      "1   0.049125    0.010094    0.127033    0.070136    0.002913    0.418519   \n",
      "2   0.015803    0.144561    0.009585    0.033477    0.021656    0.651852   \n",
      "3   0.044818    0.074384    0.032242    0.074702    0.035810    0.559259   \n",
      "4   0.047807    0.020132    0.022263    0.026900    0.012407    0.768519   \n",
      "\n",
      "   feature_98  feature_99  feature_100  feature_101  feature_106  feature_114  \\\n",
      "0    0.105263    0.036992     0.134971     0.038183        0.125     0.008813   \n",
      "1    0.105263    0.227899     0.008406     0.059721        0.000     0.038113   \n",
      "2    0.000000    0.008371     0.022958     0.061517        0.000     0.007205   \n",
      "3    0.105263    0.017050     0.066100     0.019795        0.125     0.057947   \n",
      "4    0.000000    0.016713     0.012045     0.081389        0.000     0.040607   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      2  \n",
      "4      2  \n"
     ]
    }
   ],
   "source": [
    "from Preprocessing import Tools as M_TOOL\n",
    "from Preprocessing import PCA as M_PCA\n",
    "\n",
    "from itertools import chain as ch\n",
    "\n",
    "\n",
    "df = pd.concat([df_ft, df_lb], axis=1)\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# 删除噪声\n",
    "df = M_TOOL.Drop_noise(df, if_debug=False)\n",
    "\n",
    "# 数据归一化\n",
    "df = M_TOOL.Normalization(df, if_debug=False)\n",
    "\n",
    "\n",
    "# PCA_model = M_PCA.PCA(df.values)\n",
    "# df = pd.DataFrame(PCA_model.reduce_dimension())\n",
    "# print(df.head())\n",
    "# corr = np.array(df.corr())\n",
    "# print(df.corr())\n",
    "\n",
    "\n",
    "# def flat(nest,cond_func=lambda r:type(r)==ast.Num,get_func=lambda r:r.n):\n",
    "#     cd = json.dumps(nest)\n",
    "#     t = ast.parse(cd)\n",
    "#     g = ast.walk(t)\n",
    "#     arr = list(g)\n",
    "#     arr = list(filter(cond_func,arr))\n",
    "#     arr = list(map(get_func,arr))\n",
    "#     return(arr)\n",
    "\n",
    "# 作者：navegador\n",
    "# 链接：https://www.zhihu.com/question/356442472/answer/1585424932\n",
    "# 来源：知乎\n",
    "# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "\n",
    "\n",
    "# corr_list = list(ch.from_iterable(corr))\n",
    "\n",
    "# corr_list = list(set(corr_list).difference(set([corr_list[i] for i in range(len(corr_list)-1,-1,-1) if corr_list[i] == 1.0])))\n",
    "\n",
    "# corr_list.sort()\n",
    "# # print(corr_list)\n",
    "\n",
    "# max_list = corr_list[-5:]\n",
    "# print(max_list)\n",
    "\n",
    "\n",
    "    \n",
    "# sub_df_1 = df[df['label'] == 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# print(df.shape)\n",
    "# print(sub_df_1.head())\n",
    "# print(sub_df_1.corr())\n",
    "\n",
    "print(df.shape)\n",
    "# M_TOOL.Find_useless_feature(df)\n",
    "df = M_TOOL.Delete_feature(df)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import *\n",
    "\n",
    "# m = LocallyLinearEmbedding(n_components=80)\n",
    "# m.fit(X_train, y_train)\n",
    "# X_train = m.transform(X_train)\n",
    "# X_test = m.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与预测\n",
    "\n",
    "接下来，我们将分别使用线性回归模型、决策树模型、神经网络模型、支持向量机以及 XGBoost 等分类模型对数据集进行训练，并在验证集上进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "#递归特征消除法，返回特征选择后的数据\n",
    "#参数estimator为基模型\n",
    "#参数n_features_to_select为选择的特征个数\n",
    "# print(X_train.shape)\n",
    "# X_new = RFE(estimator=LinearRegression(), n_features_to_select=50).fit_transform(X_train, y_train)\n",
    "# print(X_new.shape)\n",
    "# drop_index = M_TOOL.Find_drop_feature(X_train, X_new)\n",
    "# # print(drop_index)\n",
    "\n",
    "# X_train = np.delete(X_train, drop_index, axis=1)\n",
    "# X_test = np.delete(X_test, drop_index, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性回归模型分类准确度为 0.2375096079938509\n",
      "SKLearn 分类准确度为 0.23981552651806304\n"
     ]
    }
   ],
   "source": [
    "from Methods import LinearReg as M_LR\n",
    "# 线性回归模型\n",
    "X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df)\n",
    "\n",
    "model_liner = M_LR.LinearRegression(X_train, y_train)\n",
    "model_liner.fit()\n",
    "# model_liner.save()\n",
    "# model_liner = pk.load(open(\"./model/Model_LinearReg_2022_12_24_21_07_58.dat\", \"rb\"))    # 使用已保存的模型\n",
    "\n",
    "pre = model_liner.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "acc = Accuracy(pre, y_test)\n",
    "print(\"线性回归模型分类准确度为 {}\".format(acc))\n",
    "\n",
    "\n",
    "# 与 Sklearn 比较\n",
    "from sklearn.linear_model import LinearRegression as LR  \n",
    "model_liner_skl = LR().fit(X_train, y_train)\n",
    "pre_skl = model_liner_skl.predict(X_test)\n",
    "\n",
    "acc = Accuracy(pre_skl, y_test)\n",
    "print(\"SKLearn 分类准确度为 {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树分类准确度为 0.25211375864719443\n"
     ]
    }
   ],
   "source": [
    "# 决策树模型\n",
    "\n",
    "from Methods import DecTree as M_DTC\n",
    "X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df)\n",
    "\n",
    "parameters = {\n",
    "    \"criterion\": \"gini\",       # 选择特征的标准，分为 \"gini\" 和 \"entropy\"\n",
    "    \"splitter\": \"best\",        # 特征划分标准，分为 \"best\" 和 \"random\"\n",
    "    \"max_depth\": 11,\n",
    "    \"min_samples_split\": 2,   \n",
    "    \"min_samples_leaf\": 4,    \n",
    "    \"max_leaf_nodes\": 5000,   \n",
    "    \"min_impurity_decrease\": 0.0, \n",
    "    \"if_silent\": 1\n",
    "}\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": \"gini\",    # 选择特征的标准，分为 \"gini\" 和 \"entropy\"\n",
    "#     \"splitter\": \"best\",        # 特征划分标准，分为 \"best\" 和 \"random\"\n",
    "#     \"max_depth\": 14,\n",
    "#     \"min_samples_split\": 2,    # 叶子最小样本数\n",
    "#     \"min_samples_leaf\": 4,     # 划分最小样本数\n",
    "#     \"max_leaf_nodes\": 10000,    # 最大叶节点个数\n",
    "#     \"min_impurity_decrease\": 0.0, # 最小划分不纯度减少量\n",
    "#     \"if_silent\": 1\n",
    "# }\n",
    "\n",
    "model_dtc = M_DTC.DecisionTree(X_train, y_train)\n",
    "model_dtc.fit(parameters)\n",
    "\n",
    "pre = model_dtc.predict(X_test)\n",
    "pre = np.array(pre).reshape(-1, 1)\n",
    "\n",
    "\n",
    "acc_dtc = Accuracy(pre, y_test)\n",
    "print(\"决策树分类准确度为 {}\".format(acc_dtc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     \"criterion\": \"gini\",    # 选择特征的标准，分为 \"gini\" 和 \"entropy\"\n",
    "#     \"splitter\": \"best\",        # 特征划分标准，分为 \"best\" 和 \"random\"\n",
    "#     \"max_depth\": 14,\n",
    "#     \"min_samples_split\": 2,    # 叶子最小样本数\n",
    "#     \"min_samples_leaf\": 4,     # 划分最小样本数\n",
    "#     \"max_leaf_nodes\": 10000,    # 最大叶节点个数\n",
    "#     \"min_impurity_decrease\": 0.0, # 最小划分不纯度减少量\n",
    "#     \"if_silent\": 1\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import RegTree as M_RT\n",
    "# # 回归树模型\n",
    "\n",
    "# m, n = y_train.shape \n",
    "# y_t = np.zeros((m, n))\n",
    "# model_regtree = M_RT.RegressionTree(X_train, y_train, y_t)\n",
    "# default_parameters = {\n",
    "#     \"lamda\": 2,             # Hyperparameters\n",
    "#     \"gamma\": 1e-6,          # Hyperparameters\n",
    "#     \"gain_delta\": 0,        # The minimum gain\n",
    "#     \"max_depth\": 3,\n",
    "#     \"max_leaves\": 100,\n",
    "#     \"max_nodes\": 1000,\n",
    "#     \"min_samples\": 10,      # Minimum number of samples on a leaf\n",
    "#     \"min_feature_dif\": 5,   # The minimum number of different value for current feature\n",
    "#     \"if_silent\": 0\n",
    "# }\n",
    "# model_regtree.fit(default_parameters)\n",
    "# pre = model_regtree.predict(X_test)\n",
    "\n",
    "# acc = Accuracy(pre, Y_test)\n",
    "# print(\"回归树模型分类准确度为 {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import SVM as M_SVM\n",
    "# # 支持向量机模型\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [0, 1])\n",
    "# model_SVM_01 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_01.fit(max_times=200, ifsilent=True)\n",
    "# pre_01 = model_SVM_01.predict(X_test)\n",
    "# acc_01 = Accuracy(pre_01, y_test)\n",
    "# print(\"SVM (01) 模型分类准确度为 {}\".format(acc_01))\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [0, 2])\n",
    "# model_SVM_02 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_02.fit(max_times=200, ifsilent=True)\n",
    "# pre_02 = model_SVM_02.predict(X_test)\n",
    "# acc_02 = Accuracy(pre_02, y_test)\n",
    "# print(\"SVM (02) 模型分类准确度为 {}\".format(acc_02))\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [0, 3])\n",
    "# model_SVM_03 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_03.fit(max_times=200, ifsilent=True)\n",
    "# pre_03 = model_SVM_03.predict(X_test)\n",
    "# acc_03 = Accuracy(pre_03, y_test)\n",
    "# print(\"SVM (03) 模型分类准确度为 {}\".format(acc_03))\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [1, 2])\n",
    "# model_SVM_12 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_12.fit(max_times=200, ifsilent=True)\n",
    "# pre_12 = model_SVM_12.predict(X_test)\n",
    "# acc_12 = Accuracy(pre_12, y_test)\n",
    "# print(\"SVM (12) 模型分类准确度为 {}\".format(acc_12))\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [1, 3])\n",
    "# model_SVM_13 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_13.fit(max_times=200, ifsilent=True)\n",
    "# pre_13 = model_SVM_13.predict(X_test)\n",
    "# acc_13 = Accuracy(pre_13, y_test)\n",
    "# print(\"SVM (13) 模型分类准确度为 {}\".format(acc_13))\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df, [2, 3])\n",
    "# model_SVM_23 = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM_23.fit(max_times=200, ifsilent=True)\n",
    "# pre_23 = model_SVM_23.predict(X_test)\n",
    "# acc_23 = Accuracy(pre_23, y_test)\n",
    "# print(\"SVM (23) 模型分类准确度为 {}\".format(acc_23))\n",
    "\n",
    "\n",
    "# X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df)\n",
    "# pre_01 = model_SVM_01.predict(X_test)\n",
    "# pre_02 = model_SVM_02.predict(X_test)\n",
    "# pre_03 = model_SVM_03.predict(X_test)\n",
    "# pre_12 = model_SVM_12.predict(X_test)\n",
    "# pre_13 = model_SVM_13.predict(X_test)\n",
    "# pre_23 = model_SVM_23.predict(X_test)\n",
    "# print(pre_01)\n",
    "# print(pre_02)\n",
    "# print(pre_03)\n",
    "# print(pre_12)\n",
    "# print(pre_13)\n",
    "# print(pre_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 与 SKLearn 比较\n",
    "# from sklearn import svm \n",
    "# model = svm.SVC(kernel='linear')\n",
    "# model.fit(X_train, y_train.flatten())\n",
    "# pre_skl = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "# acc = Accuracy(pre_skl, y_test)\n",
    "# print(\"SKLearn 分类准确度为 {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import NeuralNet as M_NN\n",
    "# # 神经网络模型\n",
    "\n",
    "# model_NN = M_NN.NeuralNetwork(X_train, y_train.flatten())\n",
    "# model_NN.fit()\n",
    "# pre = model_liner.predict(X_test)\n",
    "\n",
    "# acc_1 = Accuracy(pre, Y_test)\n",
    "# print(\"线性回归模型分类准确度为 {}\".format(acc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model. feature_dim =12, label_dim =1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 20)                260       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 20)                420       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,961\n",
      "Trainable params: 1,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "244/244 [==============================] - 1s 1ms/step - loss: -192.4823 - accuracy: 0.2486 - val_loss: -1536.5481 - val_accuracy: 0.2498\n",
      "Epoch 2/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -49857.9375 - accuracy: 0.2496 - val_loss: -210327.4219 - val_accuracy: 0.2498\n",
      "Epoch 3/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -1196604.1250 - accuracy: 0.2496 - val_loss: -3359812.0000 - val_accuracy: 0.2498\n",
      "Epoch 4/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -9808397.0000 - accuracy: 0.2496 - val_loss: -20384326.0000 - val_accuracy: 0.2498\n",
      "Epoch 5/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -43050796.0000 - accuracy: 0.2496 - val_loss: -76522272.0000 - val_accuracy: 0.2498\n",
      "Epoch 6/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -138862320.0000 - accuracy: 0.2496 - val_loss: -223807248.0000 - val_accuracy: 0.2498\n",
      "Epoch 7/15\n",
      "244/244 [==============================] - 0s 987us/step - loss: -353923872.0000 - accuracy: 0.2496 - val_loss: -524270912.0000 - val_accuracy: 0.2498\n",
      "Epoch 8/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -763213952.0000 - accuracy: 0.2496 - val_loss: -1066290048.0000 - val_accuracy: 0.2498\n",
      "Epoch 9/15\n",
      "244/244 [==============================] - 0s 959us/step - loss: -1460304640.0000 - accuracy: 0.2496 - val_loss: -1958709376.0000 - val_accuracy: 0.2498\n",
      "Epoch 10/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -2611169024.0000 - accuracy: 0.2496 - val_loss: -3405935616.0000 - val_accuracy: 0.2498\n",
      "Epoch 11/15\n",
      "244/244 [==============================] - 0s 1ms/step - loss: -4362014720.0000 - accuracy: 0.2496 - val_loss: -5515826176.0000 - val_accuracy: 0.2498\n",
      "Epoch 12/15\n",
      "244/244 [==============================] - 0s 991us/step - loss: -6890923520.0000 - accuracy: 0.2496 - val_loss: -8531059712.0000 - val_accuracy: 0.2498\n",
      "Epoch 13/15\n",
      "244/244 [==============================] - 0s 984us/step - loss: -10416948224.0000 - accuracy: 0.2496 - val_loss: -12647158784.0000 - val_accuracy: 0.2498\n",
      "Epoch 14/15\n",
      "244/244 [==============================] - 0s 975us/step - loss: -15151855616.0000 - accuracy: 0.2496 - val_loss: -18134562816.0000 - val_accuracy: 0.2498\n",
      "Epoch 15/15\n",
      "244/244 [==============================] - 0s 969us/step - loss: -21340454912.0000 - accuracy: 0.2496 - val_loss: -25120892928.0000 - val_accuracy: 0.2498\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df)\n",
    "\n",
    "def deep_model(feature_dim,label_dim):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    model = Sequential()\n",
    "    print(\"create model. feature_dim ={}, label_dim ={}\".format(feature_dim, label_dim))\n",
    "    model.add(Dense(20, activation='relu', input_dim=feature_dim))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(label_dim, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_deep(X_train,y_train,X_test,y_test):\n",
    "    feature_dim = 12\n",
    "    label_dim = 1\n",
    "    model = deep_model(feature_dim,label_dim)\n",
    "    model.summary()\n",
    "    model.fit(X_train,y_train,batch_size=16, epochs=15,validation_data=(X_test,y_test))\n",
    "    \n",
    "train_deep(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 模型分类准确度为 0.2805534204458109\n"
     ]
    }
   ],
   "source": [
    "from Methods import XGBoost as M_XGB\n",
    "# XGBoost 模型\n",
    "X_train, y_train, X_test, y_test = M_TOOL.random_Split_Data_Label(df)\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": 0.3,       # 如同学习率\n",
    "    \"min_child_weight\": 1, \n",
    "    # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "    #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "    # 这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "    \"max_depth\": 9,             # 构建树的深度，越大越容易过拟合\n",
    "    \"gamma\": 0,                 # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "    \"subsample\": 1,             # 随机采样训练样本 训练实例的子采样比\n",
    "    \"max_delta_step\": 0,        # 最大增量步长，我们允许每个树的权重估计。\n",
    "    \"colsample_bytree\": 1,      # 生成树时进行的列采样 \n",
    "    \"reg_lambda\": 1,            # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    \"reg_alpha\": 0,             # L1 正则项参数\n",
    "    \"objective\": 'multi:softmax',   # 多分类的问题 指定学习任务和相应的学习目标 binary:logitraw   multi:softmax\n",
    "    \"num_class\": 4,             # 类别数\n",
    "    \"n_estimators\": 90,         # 树的个数\n",
    "    \"seed\": 2000,                # 随机种子     \n",
    "    \"if_silent\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "model_XGB = M_XGB.XGBoost(X_train, y_train)\n",
    "model_XGB.fit(parameters)\n",
    "# model_XGB.save(\"./model/Model_XGBoost_0.28\")\n",
    "# model_XGB= pk.load(open(\"./model/Model_XGBoost_0.28\", \"rb\")) \n",
    "\n",
    "pre = model_XGB.predict(X_test)\n",
    "pre = np.array(pre).reshape(-1, 1)\n",
    "\n",
    "acc_xgb = Accuracy(pre, y_test)\n",
    "print(\"XGBoost 模型分类准确度为 {}\".format(acc_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
