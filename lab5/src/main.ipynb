{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd \n",
    "import pickle as pk\n",
    "\n",
    "from Judge.Score import *   # 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = pd.read_csv('./Datasets/train_feature.csv')\n",
    "df_lb = pd.read_csv('./Datasets/train_label.csv') \n",
    "\n",
    "# df_origin = pd.concat([df_ft_origin, df_lb_origin],axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "首先进行数据的预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(df_ft_origin.head())\n",
    "# print(df_ft_origin['feature_0'])\n",
    "# print(type(df_ft_origin['feature_0']))\n",
    "# print(df_ft_origin.mean())\n",
    "# print(df_ft_origin.median())\n",
    "# print(df_ft_origin.var())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5195, 121)\n"
     ]
    }
   ],
   "source": [
    "from Preprocessing import Tools as M_TOOL\n",
    "from Preprocessing import PCA as M_PCA\n",
    "\n",
    "from itertools import chain as ch\n",
    "\n",
    "\n",
    "df = pd.concat([df_ft, df_lb], axis=1)\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# 删除噪声\n",
    "df = M_TOOL.Drop_noise(df, if_debug=False)\n",
    "\n",
    "# 数据归一化\n",
    "df = M_TOOL.Normalization(df, if_debug=False)\n",
    "\n",
    "\n",
    "# PCA_model = M_PCA.PCA(df.values)\n",
    "# df = pd.DataFrame(PCA_model.reduce_dimension())\n",
    "# print(df.head())\n",
    "# corr = np.array(df.corr())\n",
    "# print(df.corr())\n",
    "\n",
    "\n",
    "# def flat(nest,cond_func=lambda r:type(r)==ast.Num,get_func=lambda r:r.n):\n",
    "#     cd = json.dumps(nest)\n",
    "#     t = ast.parse(cd)\n",
    "#     g = ast.walk(t)\n",
    "#     arr = list(g)\n",
    "#     arr = list(filter(cond_func,arr))\n",
    "#     arr = list(map(get_func,arr))\n",
    "#     return(arr)\n",
    "\n",
    "# 作者：navegador\n",
    "# 链接：https://www.zhihu.com/question/356442472/answer/1585424932\n",
    "# 来源：知乎\n",
    "# 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "\n",
    "\n",
    "\n",
    "# corr_list = list(ch.from_iterable(corr))\n",
    "\n",
    "# corr_list = list(set(corr_list).difference(set([corr_list[i] for i in range(len(corr_list)-1,-1,-1) if corr_list[i] == 1.0])))\n",
    "\n",
    "# corr_list.sort()\n",
    "# # print(corr_list)\n",
    "\n",
    "# max_list = corr_list[-5:]\n",
    "# print(max_list)\n",
    "\n",
    "\n",
    "    \n",
    "# sub_df_1 = df[df['label'] == 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# print(df.shape)\n",
    "# print(sub_df_1.head())\n",
    "# print(sub_df_1.corr())\n",
    "\n",
    "print(df.shape)\n",
    "X_train, y_train, X_test, Y_test = M_TOOL.random_Split_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import *\n",
    "\n",
    "# m = LocallyLinearEmbedding(n_components=80)\n",
    "# m.fit(X_train, y_train)\n",
    "# X_train = m.transform(X_train)\n",
    "# X_test = m.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与预测\n",
    "\n",
    "接下来，我们将分别使用线性回归模型、决策树模型、神经网络模型、支持向量机以及 XGBoost 等分类模型对数据集进行训练，并在验证集上进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3896, 120)\n",
      "(3896, 50)\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0.423077   0.621537   0.064280   0.333333   0.384615   0.696370   \n",
      "1   0.365385   0.210505   0.590200   0.777778   0.153846   0.465347   \n",
      "2   0.576923   0.487735   0.173248   0.555556   0.076923   0.828383   \n",
      "3   0.461538   0.556613   0.004739   0.555556   0.230769   0.613861   \n",
      "4   0.442308   0.358241   0.709728   0.666667   0.230769   0.792079   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_111  feature_112  \\\n",
      "0   0.367347        0.5        0.1   0.040544  ...     0.098502     0.340426   \n",
      "1   0.428571        0.4        0.0   0.049125  ...     0.049627     0.723404   \n",
      "2   0.408163        0.3        0.0   0.015803  ...     0.063077     0.723404   \n",
      "3   0.244898        0.5        0.0   0.044818  ...     0.099858     0.680851   \n",
      "4   0.612245        0.3        0.0   0.047807  ...     0.090963     0.553191   \n",
      "\n",
      "   feature_113  feature_114  feature_115  feature_116  feature_117  \\\n",
      "0     0.027512     0.008813     0.000000     0.391304     0.028634   \n",
      "1     0.852755     0.038113     0.111111     0.195652     0.009939   \n",
      "2     0.907037     0.007205     0.222222     0.521739     0.035199   \n",
      "3     0.164809     0.057947     0.111111     0.586957     0.007844   \n",
      "4     0.600396     0.040607     0.777778     0.500000     0.019625   \n",
      "\n",
      "   feature_118  feature_119  label  \n",
      "0          0.2     0.520833      0  \n",
      "1          0.2     0.354167      0  \n",
      "2          0.1     0.395833      0  \n",
      "3          0.2     0.229167      2  \n",
      "4          0.2     0.479167      2  \n",
      "\n",
      "[5 rows x 121 columns]\n",
      "    feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0    0.423077   0.621537   0.064280   0.333333   0.384615   0.696370   \n",
      "1    0.365385   0.210505   0.590200   0.777778   0.153846   0.465347   \n",
      "2    0.576923   0.487735   0.173248   0.555556   0.076923   0.828383   \n",
      "14   0.673077   0.379062   0.840976   0.555556   0.307692   0.491749   \n",
      "17   0.596154   0.287010   0.815918   0.333333   0.384615   0.765677   \n",
      "\n",
      "    feature_6  feature_7  feature_8  feature_9  ...  feature_111  feature_112  \\\n",
      "0    0.367347        0.5       0.10   0.040544  ...     0.098502     0.340426   \n",
      "1    0.428571        0.4       0.00   0.049125  ...     0.049627     0.723404   \n",
      "2    0.408163        0.3       0.00   0.015803  ...     0.063077     0.723404   \n",
      "14   0.653061        0.3       0.25   0.031436  ...     0.115637     0.468085   \n",
      "17   0.306122        0.5       0.15   0.075815  ...     0.083129     0.085106   \n",
      "\n",
      "    feature_113  feature_114  feature_115  feature_116  feature_117  \\\n",
      "0      0.027512     0.008813     0.000000     0.391304     0.028634   \n",
      "1      0.852755     0.038113     0.111111     0.195652     0.009939   \n",
      "2      0.907037     0.007205     0.222222     0.521739     0.035199   \n",
      "14     0.991166     0.031383     0.333333     0.500000     0.056184   \n",
      "17     0.327775     0.023650     0.333333     0.391304     0.012325   \n",
      "\n",
      "    feature_118  feature_119  label  \n",
      "0           0.2     0.520833      0  \n",
      "1           0.2     0.354167      0  \n",
      "2           0.1     0.395833      0  \n",
      "14          0.3     0.354167      0  \n",
      "17          0.2     0.666667      0  \n",
      "\n",
      "[5 rows x 121 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "#递归特征消除法，返回特征选择后的数据\n",
    "#参数estimator为基模型\n",
    "#参数n_features_to_select为选择的特征个数\n",
    "print(X_train.shape)\n",
    "X_new = RFE(estimator=LinearRegression(), n_features_to_select=50).fit_transform(X_train, y_train)\n",
    "print(X_new.shape)\n",
    "drop_index = M_TOOL.Find_drop_feature(X_train, X_new)\n",
    "# print(drop_index)\n",
    "\n",
    "X_train = np.delete(X_train, drop_index, axis=1)\n",
    "X_test = np.delete(X_test, drop_index, axis=1)\n",
    "\n",
    "\n",
    "M_TOOL.Find_useless_feature(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线性回归模型分类准确度为 0.23787528868360275\n",
      "SKLearn 分类准确度为 0.2424942263279446\n"
     ]
    }
   ],
   "source": [
    "from Methods import LinearReg as M_LR\n",
    "# 线性回归模型\n",
    "\n",
    "model_liner = M_LR.LinearRegression(X_train, y_train)\n",
    "model_liner.fit()\n",
    "# model_liner.save()\n",
    "# model_liner = pk.load(open(\"./model/Model_LinearReg_2022_12_24_21_07_58.dat\", \"rb\"))    # 使用已保存的模型\n",
    "\n",
    "pre = model_liner.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "acc = Accuracy(pre, Y_test)\n",
    "print(\"线性回归模型分类准确度为 {}\".format(acc))\n",
    "\n",
    "\n",
    "# 与 Sklearn 比较\n",
    "from sklearn.linear_model import LinearRegression as LR  \n",
    "model_liner_skl = LR().fit(X_train, y_train)\n",
    "pre_skl = model_liner_skl.predict(X_test)\n",
    "\n",
    "acc = Accuracy(pre_skl, Y_test)\n",
    "print(\"SKLearn 分类准确度为 {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 决策树模型\n",
    "\n",
    "# from Methods import DecTree as M_DTC\n",
    "# parameters = {\n",
    "#     \"criterion\": \"gini\",    # 选择特征的标准，分为 \"gini\" 和 \"entropy\"\n",
    "#     \"splitter\": \"best\",        # 特征划分标准，分为 \"best\" 和 \"random\"\n",
    "#     \"max_depth\": 8,\n",
    "#     \"min_samples_split\": 2,    # 叶子最小样本数\n",
    "#     \"min_samples_leaf\": 1,     # 划分最小样本数\n",
    "#     \"max_leaf_nodes\": 10000,    # 最大叶节点个数\n",
    "#     \"min_impurity_decrease\": 0.0, # 最小划分不纯度减少量\n",
    "#     \"if_silent\": 1\n",
    "# }\n",
    "\n",
    "# model_dtc = M_DTC.DecisionTree(X_train, y_train)\n",
    "# model_dtc.fit(parameters)\n",
    "\n",
    "# pre = model_dtc.predict(X_test)\n",
    "# pre = np.array(pre).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# acc_dtc = Accuracy(pre, Y_test)\n",
    "# print(\"决策树分类准确度为 {}\".format(acc_dtc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import RegTree as M_RT\n",
    "# # 回归树模型\n",
    "\n",
    "# m, n = y_train.shape \n",
    "# y_t = np.zeros((m, n))\n",
    "# model_regtree = M_RT.RegressionTree(X_train, y_train, y_t)\n",
    "# default_parameters = {\n",
    "#     \"lamda\": 2,             # Hyperparameters\n",
    "#     \"gamma\": 1e-6,          # Hyperparameters\n",
    "#     \"gain_delta\": 0,        # The minimum gain\n",
    "#     \"max_depth\": 3,\n",
    "#     \"max_leaves\": 100,\n",
    "#     \"max_nodes\": 1000,\n",
    "#     \"min_samples\": 10,      # Minimum number of samples on a leaf\n",
    "#     \"min_feature_dif\": 5,   # The minimum number of different value for current feature\n",
    "#     \"if_silent\": 0\n",
    "# }\n",
    "# model_regtree.fit(default_parameters)\n",
    "# pre = model_regtree.predict(X_test)\n",
    "\n",
    "# acc = Accuracy(pre, Y_test)\n",
    "# print(\"回归树模型分类准确度为 {}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import SVM as M_SVM\n",
    "# # 支持向量机模型\n",
    "\n",
    "# model_SVM = M_SVM.SupportVectorMachine(X_train, y_train)\n",
    "# model_SVM.fit(max_times=500, ifsilent=True)\n",
    "# pre = model_SVM.predict(X_test)\n",
    "\n",
    "# acc = Accuracy(pre, Y_test)\n",
    "# print(\"SVM 模型分类准确度为 {}\".format(acc))\n",
    "\n",
    "\n",
    "# # 与 SKLearn 比较\n",
    "# from sklearn import svm \n",
    "# model = svm.SVC(kernel='linear')\n",
    "# model.fit(X_train, y_train.flatten())\n",
    "# pre_skl = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "# acc = Accuracy(pre_skl, Y_test)\n",
    "# print(\"SKLearn 分类准确度为 {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Methods import NeuralNet as M_NN\n",
    "# # 神经网络模型\n",
    "\n",
    "# model_NN = M_NN.NeuralNetwork(X_train, y_train.flatten())\n",
    "# model_NN.fit()\n",
    "# pre = model_liner.predict(X_test)\n",
    "\n",
    "# acc_1 = Accuracy(pre, Y_test)\n",
    "# print(\"线性回归模型分类准确度为 {}\".format(acc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 模型分类准确度为 0.2517321016166282\n"
     ]
    }
   ],
   "source": [
    "from Methods import XGBoost as M_XGB\n",
    "# XGBoost 模型\n",
    "\n",
    "model_XGB = M_XGB.XGBoost(X_train, y_train)\n",
    "\n",
    "model_XGB.fit(T=100, max_depth=8)\n",
    "pre = model_XGB.predict(X_test)\n",
    "pre = np.array(pre).reshape(-1, 1)\n",
    "\n",
    "acc_xgb = Accuracy(pre, Y_test)\n",
    "print(\"XGBoost 模型分类准确度为 {}\".format(acc_xgb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
